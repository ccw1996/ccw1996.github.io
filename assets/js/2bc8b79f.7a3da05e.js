"use strict";(self.webpackChunkblog_sample=self.webpackChunkblog_sample||[]).push([[8372],{3149:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>c,frontMatter:()=>a,metadata:()=>l,toc:()=>p});var o=t(5893),i=t(1151);const a={title:"clip\u5bfc\u51fa\u4e3aonnx",description:"clip\u5bfc\u51fa\u4e3aonnx",tags:["work"],editor:"caroot"},r=void 0,l={id:"LLM/export_clip_onnx",title:"clip\u5bfc\u51fa\u4e3aonnx",description:"clip\u5bfc\u51fa\u4e3aonnx",source:"@site/docs/LLM/export_clip_onnx.md",sourceDirName:"LLM",slug:"/LLM/export_clip_onnx",permalink:"/docs/LLM/export_clip_onnx",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/edit/main/website/docs/LLM/export_clip_onnx.md",tags:[{label:"work",permalink:"/docs/tags/work"}],version:"current",lastUpdatedAt:1714915092,formattedLastUpdatedAt:"May 5, 2024",frontMatter:{title:"clip\u5bfc\u51fa\u4e3aonnx",description:"clip\u5bfc\u51fa\u4e3aonnx",tags:["work"],editor:"caroot"},sidebar:"tutorialSidebar",previous:{title:"DINOv2",permalink:"/docs/LLM/Dinov2"},next:{title:"\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u65f6\u95f4\uff1a\u4ece\u4f30\u7b97\u5230 FLOPs \u63a8\u5bfc",permalink:"/docs/LLM/flop_calc"}},s={},p=[{value:"\u9488\u5bf9image\u548ctext\u5355\u72ec\u63d0\u53d6onnx",id:"\u9488\u5bf9image\u548ctext\u5355\u72ec\u63d0\u53d6onnx",level:2},{value:"\u5408\u5e76\u6210\u4e00\u4e2a\u5b8c\u6574\u7684onnx",id:"\u5408\u5e76\u6210\u4e00\u4e2a\u5b8c\u6574\u7684onnx",level:2}];function d(e){const n={code:"code",h2:"h2",pre:"pre",...(0,i.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"\u9488\u5bf9image\u548ctext\u5355\u72ec\u63d0\u53d6onnx",children:"\u9488\u5bf9image\u548ctext\u5355\u72ec\u63d0\u53d6onnx"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport clip\nfrom PIL import Image\n \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n \nmodel.float()\nmodel.eval()\n \nimage = preprocess(Image.open(\"clip_dog.png\")).unsqueeze(0).to(device)\ntext = clip.tokenize([\"a dog\", \"a cat\"]).to(device)\n \nprint(\"text:\", text)\n \nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n \n    logits_per_image, logits_per_text = model(image, text)\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n \nprint(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]\n \n# export to ONNX\n \n \nclass ImgModelWrapper(nn.Module):\n    def __init__(self):\n        super(ImgModelWrapper, self).__init__()\n        self.model = model\n \n    def forward(self, image):\n        image_features = model.encode_image(image)\n        return image_features\n \n \nclass TxtModelWrapper(nn.Module):\n    def __init__(self):\n        super(TxtModelWrapper, self).__init__()\n        self.model = model\n \n    def forward(self, image):\n        text_features = model.encode_text(text)\n        return text_features\n \n \nimg_model = ImgModelWrapper()\ntxt_model = TxtModelWrapper()\n \ntorch.onnx.export(img_model,               # model being run\n                  image,                         # model input (or a tuple for multiple inputs)\n                  \"openai_vit_img.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=12,          # the ONNX version to export the model to\n                  do_constant_folding=False,  # whether to execute constant folding for optimization\n                  input_names=['input'],   # the model's input names\n                  output_names=['output'],  # the model's output names\n                  dynamic_axes={'input': {0: 'batch'}})\ntorch.onnx.export(txt_model,               # model being run\n                  text,                         # model input (or a tuple for multiple inputs)\n                  \"openai_vit_txt.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=12,          # the ONNX version to export the model to\n                  do_constant_folding=False,  # whether to execute constant folding for optimization\n                  input_names=['input'],   # the model's input names\n                  output_names=['output'],  # the model's output names\n                  dynamic_axes={'input': {0: 'batch'}})\n"})}),"\n",(0,o.jsx)(n.h2,{id:"\u5408\u5e76\u6210\u4e00\u4e2a\u5b8c\u6574\u7684onnx",children:"\u5408\u5e76\u6210\u4e00\u4e2a\u5b8c\u6574\u7684onnx"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import clip\nimport torch\nfrom PIL import Image\n\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nmodel, preprocess = clip.load("ViT-B/32", device=device)\n \nmodel.float()\nmodel.eval()\nnpx=model.visual.input_resolution\nimage = preprocess(Image.open("clip_dog.png")).unsqueeze(0).to(device)\ntext = clip.tokenize(["a dog", "a cat"]).to(device)\n\nmodel.forward(image,text)\ntorch.onnx.export(model,(image,text),"clip.onnx",export_params=True,input_names=["IMAGE", "TEXT"],\n  output_names=["LOGITS_PER_IMAGE", "LOGITS_PER_TEXT"],\n  opset_version=14,\n  dynamic_axes={\n      "IMAGE": {\n          0: "image_batch_size",\n      },\n      "TEXT": {\n          0: "text_batch_size",\n      },\n      "LOGITS_PER_IMAGE": {\n          0: "image_batch_size",\n          1: "text_batch_size",\n      },\n      "LOGITS_PER_TEXT": {\n          0: "text_batch_size",\n          1: "image_batch_size",\n      },\n  })\n'})})]})}function c(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>l,a:()=>r});var o=t(7294);const i={},a=o.createContext(i);function r(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);