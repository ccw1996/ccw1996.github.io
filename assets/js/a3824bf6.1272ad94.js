"use strict";(self.webpackChunkblog_sample=self.webpackChunkblog_sample||[]).push([[4395],{2699:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"LLM/paper/codeVQA","title":"Modular visual question answering via code generation","description":"Modular visual question answering via code generation","source":"@site/docs/LLM/paper/codeVQA.md","sourceDirName":"LLM/paper","slug":"/LLM/paper/codeVQA","permalink":"/docs/LLM/paper/codeVQA","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/edit/main/website/docs/LLM/paper/codeVQA.md","tags":[{"inline":true,"label":"paper","permalink":"/docs/tags/paper"}],"version":"current","lastUpdatedAt":1740785860000,"frontMatter":{"title":"Modular visual question answering via code generation","description":"Modular visual question answering via code generation","tags":["paper"],"editor":"caroot"},"sidebar":"tutorialSidebar","previous":{"title":"Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers","permalink":"/docs/LLM/paper/Transformer-MM-Explainability"},"next":{"title":"cpp","permalink":"/docs/cpp/"}}');var a=n(4848),o=n(8453);const s={title:"Modular visual question answering via code generation",description:"Modular visual question answering via code generation",tags:["paper"],editor:"caroot"},r=void 0,c={},d=[{value:"\u4ecb\u7ecd",id:"\u4ecb\u7ecd",level:2}];function l(e){const t={a:"a",h2:"h2",img:"img",li:"li",p:"p",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"\u4e3a\u4e86\u63d0\u9ad8\u6d89\u53ca\u590d\u6742\u63a8\u7406\u7684 VQA \u793a\u4f8b\u7684\u51c6\u786e\u6027:"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2306.05392",children:"Modular Visual Question Answering via Code Generation \u8bba\u6587"})}),"\n",(0,a.jsx)(t.p,{children:"\u5177\u4f53\u6765\u8bf4\uff0c\u5f53\u7ed9\u5b9a\u6709\u5173\u56fe\u50cf\u6216\u4e00\u7ec4\u56fe\u50cf\u7684\u95ee\u9898\u65f6\uff0cCodeVQA \u4f1a\u751f\u6210\u4e00\u4e2a\u5177\u6709\u7b80\u5355\u89c6\u89c9\u51fd\u6570\u7684 Python \u7a0b\u5e8f\uff08\u4ee3\u7801\uff09\uff0c\u5141\u8bb8\u5b83\u5904\u7406\u56fe\u50cf\uff0c\u5e76\u6267\u884c\u8be5\u7a0b\u5e8f\u4ee5\u786e\u5b9a\u7b54\u6848\u3002"}),"\n",(0,a.jsx)(t.h2,{id:"\u4ecb\u7ecd",children:"\u4ecb\u7ecd"}),"\n",(0,a.jsx)(t.p,{children:"We instantiate the CodeVQA framework using three visual functions: (1) , (2) , and (3) ."}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Query, which answers a question about a single image, is implemented using the few-shot Plug-and-Play VQA (PnP-VQA) method. PnP-VQA generates captions using BLIP \u2014 an image-captioning transformer pre-trained on millions of image-caption pairs \u2014 and feeds these into a LLM that outputs the answers to the question."}),"\n",(0,a.jsx)(t.li,{children:"Get_pos, which is an object localizer that takes a description of an object as input and returns its position in the image, is implemented using GradCAM. Specifically, the description and the image are passed through the BLIP joint text-image encoder, which predicts an image-text matching score. GradCAM takes the gradient of this score with respect to the image features to find the region most relevant to the text."}),"\n",(0,a.jsx)(t.li,{children:"Find_matching_image, which is used in multi-image questions to find the image that best matches a given input phrase, is implemented by using BLIP text and image encoders to compute a text embedding for the phrase and an image embedding for each image. Then the dot products of the text embedding with each image embedding represent the relevance of each image to the phrase, and we pick the image that maximizes this relevance."}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s16000/image2.gif",alt:""})})]})}function p(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>r});var i=n(6540);const a={},o=i.createContext(a);function s(e){const t=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(o.Provider,{value:t},e.children)}}}]);