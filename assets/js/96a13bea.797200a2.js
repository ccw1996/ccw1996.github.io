"use strict";(self.webpackChunkblog_sample=self.webpackChunkblog_sample||[]).push([[1617],{6531:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>s,metadata:()=>n,toc:()=>p});const n=JSON.parse('{"id":"24-08/TensorRT Model Optimizer\u91cf\u5316\u548c\u6a21\u578b\u5bfc\u51fa","title":"TensorRT Model Optimizer\u91cf\u5316\u548c\u6a21\u578b\u5bfc\u51fa","description":"","source":"@site/docs/24-08/TensorRT Model Optimizer\u91cf\u5316\u548c\u6a21\u578b\u5bfc\u51fa.md","sourceDirName":"24-08","slug":"/24-08/TensorRT Model Optimizer\u91cf\u5316\u548c\u6a21\u578b\u5bfc\u51fa","permalink":"/docs/24-08/TensorRT Model Optimizer\u91cf\u5316\u548c\u6a21\u578b\u5bfc\u51fa","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/edit/main/website/docs/24-08/TensorRT Model Optimizer\u91cf\u5316\u548c\u6a21\u578b\u5bfc\u51fa.md","tags":[{"inline":true,"label":"profile","permalink":"/docs/tags/profile"},{"inline":true,"label":"tensorrt","permalink":"/docs/tags/tensorrt"},{"inline":true,"label":"optimize","permalink":"/docs/tags/optimize"}],"version":"current","lastUpdatedAt":1740785860000,"frontMatter":{"title":"TensorRT Model Optimizer\u91cf\u5316\u548c\u6a21\u578b\u5bfc\u51fa","tags":["profile","tensorrt","optimize"]},"sidebar":"tutorialSidebar","previous":{"title":"24-08","permalink":"/docs/24-08/"},"next":{"title":"24-11","permalink":"/docs/24-11/"}}');var r=o(4848),i=o(8453);const s={title:"TensorRT Model Optimizer\u91cf\u5316\u548c\u6a21\u578b\u5bfc\u51fa",tags:["profile","tensorrt","optimize"]},a=void 0,l={},p=[];function m(e){const t={code:"code",pre:"pre",...(0,i.R)(),...e.components};return(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'from transformers import AutoImageProcessor, ResNetForImageClassification\nimport torch\nfrom diffusers.utils import load_image\nimport modelopt.torch.quantization as mtq\n \n# https://huggingface.co/microsoft/resnet-50\nprocessor = AutoImageProcessor.from_pretrained("resnet_50")\nmodel = ResNetForImageClassification.from_pretrained("resnet_50")\n \nimg_url = "cat1.jpg"\n \nimage = load_image(img_url).resize((512, 512))\ninputs = processor(image, return_tensors="pt")\n \npixel_values = inputs["pixel_values"]\ndata_loader=[pixel_values]\n \ndef forward_loop(model):\n    for batch in data_loader:\n        model(batch)\n \n# mtq.INT8_SMOOTHQUANT_CFG\n# Quantize the model and perform calibration (PTQ)\nmodel = mtq.quantize(model, mtq.INT8_DEFAULT_CFG, forward_loop)\n \ntorch.onnx.export(model,  # model being run\n                  (pixel_values),  # model input (or a tuple for multiple inputs)\n                  "resnet50_quant.onnx",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=15,          # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names=[\'pixel_values\'],   # the model\'s input names\n                  output_names=[\'output\'],  # the model\'s output names\n                  dynamic_axes={\'pixel_values\': {0: \'batch\'}},\n                  )\n'})})}function d(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},8453:(e,t,o)=>{o.d(t,{R:()=>s,x:()=>a});var n=o(6540);const r={},i=n.createContext(r);function s(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),n.createElement(i.Provider,{value:t},e.children)}}}]);