"use strict";(self.webpackChunkblog_sample=self.webpackChunkblog_sample||[]).push([[3997],{8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>s});var a=n(6540);const o={},r=a.createContext(o);function i(e){const t=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(r.Provider,{value:t},e.children)}},9258:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"ml/compiler/triton/lower","title":"\u7aa5\u63a2Triton\u7684lower","description":"https://zhuanlan.zhihu.com/p/695171704","source":"@site/docs/ml/compiler/triton/lower.md","sourceDirName":"ml/compiler/triton","slug":"/ml/compiler/triton/lower","permalink":"/docs/ml/compiler/triton/lower","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/edit/main/website/docs/ml/compiler/triton/lower.md","tags":[{"inline":true,"label":"work","permalink":"/docs/tags/work"},{"inline":true,"label":"interview","permalink":"/docs/tags/interview"}],"version":"current","lastUpdatedAt":1740785860000,"frontMatter":{"title":"\u7aa5\u63a2Triton\u7684lower","tags":["work","interview"]},"sidebar":"tutorialSidebar","previous":{"title":"CPU Cache\u7684\u6d4b\u91cf\u65b9\u6cd5","permalink":"/docs/ml/cache"},"next":{"title":"pytorch\u5377\u79ef\u5c42\u57fa\u7840\u4e03\u95ee","permalink":"/docs/ml/conv"}}');var o=n(4848),r=n(8453);const i={title:"\u7aa5\u63a2Triton\u7684lower",tags:["work","interview"]},s="example",l={},d=[{value:"make_backend &amp; add_stage",id:"make_backend--add_stage",level:2},{value:"load_dialects",id:"load_dialects",level:2},{value:"make_ir",id:"make_ir",level:2},{value:"make_ttir",id:"make_ttir",level:2},{value:"make_ttgir",id:"make_ttgir",level:2},{value:"make_llir",id:"make_llir",level:2}];function c(e){const t={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(t.blockquote,{children:["\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.a,{href:"https://zhuanlan.zhihu.com/p/695171704",children:"https://zhuanlan.zhihu.com/p/695171704"}),"\n",(0,o.jsx)(t.a,{href:"https://zhuanlan.zhihu.com/p/695255185",children:"https://zhuanlan.zhihu.com/p/695255185"}),"\n",(0,o.jsx)(t.a,{href:"https://zhuanlan.zhihu.com/p/696133729",children:"https://zhuanlan.zhihu.com/p/696133729"})]}),"\n"]}),"\n",(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"example",children:"example"})}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'import triton\nimport triton.language as tl\n\n@triton.jit\ndef addi_kernel(x_ptr,  # *Pointer* to first input vector.\n               output_ptr,  # *Pointer* to output vector.\n               n_elements,  # Size of the vector.\n               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n               ):\n    x = tl.load(x_ptr)\n    output = x + 1\n    tl.store(output_ptr, output)\nsrc = triton.compiler.ASTSource(fn=addi_kernel, signature="*i32,*i32", constants={"n_elements": 1,"BLOCK_SIZE": 1})\nret = triton.compile(src, target=("cuda", 80))\nfor k in [\'ttir\', \'ttgir\', \'llir\']:\n    print(ret.asm[k])\n'})}),"\n",(0,o.jsx)(t.p,{children:"\u9996\u5148triton.compile\u51fd\u6570\u5f00\u59cb\u3002\u5c06\u4e0a\u8ff0kernel\u5c01\u88c5\u6210\u4e00\u4e2aASTSource\u540e\uff0c\u4f1a\u8fdb\u5165\u4ee3\u7801\u7684python/triton/compiler/compiler.py/compile\u51fd\u6570\uff0c\u8fd9\u91cc\u4e00\u4e9b\u4e0d\u592a\u91cd\u8981\u7684\u529f\u80fd\u7528\u7701\u7565\u53f7\u8df3\u8fc7\u4e86\uff0c\u6bd4\u5982\u5229\u7528cache\u7f16\u8bd1\u3001\u89e3\u6790\u9009\u9879\u7b49\u7b49\u3002"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:"def compile(src, target=None, options=None):\n    ......\n    backend = make_backend(target) # \u6839\u636e\u6307\u5b9a\u7684target\u83b7\u53d6\u4e00\u4e2abackend\uff0c\u8fd9\u91cc\u4f1a\u8fd4\u56deCUDABackend\n    ......\n    backend.add_stages(stages, options) # \u6dfb\u52a0\u7f16\u8bd1stage\n    ......\n    ir.load_dialects(context)  # \u52a0\u8f7d\u6240\u9700dialects\n    backend.load_dialects(context)\n    module = src.make_ir(options, context) # \u521b\u5efaIR\n    ......\n    for ext, compile_ir in list(stages.items())[first_stage:]:\n        next_module = compile_ir(module, metadata) # \u7f16\u8bd1\u5404\u4e2a\u9636\u6bb5IR\n        ......\n        module = next_module\n    ......\n    return CompiledKernel(src, metadata_group, hash)\n"})}),"\n",(0,o.jsx)(t.p,{children:"\u8fd9\u90e8\u5206\u662f\u7f16\u8bd1\u7684\u6838\u5fc3\u4ee3\u7801\uff0c\u4e0a\u9762\u7684\u6b65\u9aa4\u53ef\u4ee5\u5206\u6210\u4e94\u4e2a\u4e3b\u8981\u9636\u6bb5\uff1amake_backend\u3001add_stage\u3001load_dialects\u3001make_ir\u3001compile_ir\u3002"}),"\n",(0,o.jsx)(t.h2,{id:"make_backend--add_stage",children:"make_backend & add_stage"}),"\n",(0,o.jsx)(t.p,{children:"\u524d\u4e24\u4e2a\u9636\u6bb5\u7684\u5185\u5bb9\u6bd4\u8f83\u597d\u7406\u89e3\uff0cmake_backend\u9636\u6bb5\u6211\u4eec\u4f1a\u901a\u8fc7\u6307\u5b9a\u7684Target\u540d\u5b57\u83b7\u53d6\u4e00\u4e2aCUDABackend\uff0c\u8fd9\u5176\u4e2d\u8fd8\u5305\u62ec\u5bf9\u5e94driver\u521d\u59cb\u5316\u7684\u4e00\u4e9b\u6d41\u7a0b"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'def add_stages(self, stages, options):\n        stages["ttir"] = lambda src, metadata: self.make_ttir(src, metadata, options)\n        stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, self.capability)\n        stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options, self.capability)\n        stages["ptx"] = lambda src, metadata: self.make_ptx(src, metadata, options, self.capability)\n        stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.capability)\n'})}),"\n",(0,o.jsx)(t.h2,{id:"load_dialects",children:"load_dialects"}),"\n",(0,o.jsx)(t.p,{children:"\u8fd9\u662f\u6211\u4eec\u9700\u8981\u7740\u91cdfollow\u7684\u4e00\u4e2a\u6b65\u9aa4\uff0c\u4e3b\u8981\u7528\u6765\u7406\u89e3dialect\u3002\u6839\u636e\u4e0a\u9762\u7684\u4ee3\u7801\u6211\u4eec\u53d1\u73b0\u8fd9\u91cc\u6709\u4e24\u4e2a\u6b65\u9aa4\uff1air.load_dialects\u548cbackend.load_dialects\uff0c\u9996\u5148\u5173\u6ce8\u7b2c\u4e00\u4e2a\u3002"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'m.def("load_dialects", [](MLIRContext &context) {\n    DialectRegistry registry;\n    registry.insert<TritonDialect, ::mlir::triton::gpu::TritonGPUDialect,\n                    math::MathDialect, arith::ArithDialect, index::IndexDialect,\n                    scf::SCFDialect, ::mlir::gpu::GPUDialect,\n                    cf::ControlFlowDialect, LLVM::LLVMDialect>();\n    registerBuiltinDialectTranslation(registry);\n    registerLLVMDialectTranslation(registry);\n    context.appendDialectRegistry(registry);\n    context.loadAllAvailableDialects();\n  });\n'})}),"\n",(0,o.jsx)(t.h2,{id:"make_ir",children:"make_ir"}),"\n",(0,o.jsx)(t.p,{children:"triton\u652f\u6301\u6e90\u4ee3\u7801\u8f93\u5165\u548cMLIR\u8f93\u5165\u3002\u5982\u679c\u662fMLIR\uff0c\u5b83\u7684\u8c03\u7528\u7ecf\u8fc7make_ir->parse_mlir_module->parseSourceFile->parseAsmSourceFile->TopLevelOperationParser::parse\u5230\u8fbe\u4e00\u4e2a\u89e3\u6790\u5668\uff0c\u6839\u636etoken\u7684\u7c7b\u578b\u6267\u884c\u524d\u7aef\u7684\u89e3\u6790\u3002"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-cpp",children:"ParseResult TopLevelOperationParser::parse(Block *topLevelBlock,\n                                           Location parserLoc) {\n  // Create a top-level operation to contain the parsed state.\n  OwningOpRef<ModuleOp> topLevelOp(ModuleOp::create(parserLoc));\n  OperationParser opParser(state, topLevelOp.get());\n  while (true) {\n    switch (getToken().getKind()) {\n    default:\n      // Parse a top-level operation.\n      if (opParser.parseOperation())\n        return failure();\n      break;\n\n    // If we got to the end of the file, then we're done.\n    case Token::eof: {\n      if (opParser.finalize())\n        return failure();\n     .......\n\n  }\n}\n"})}),"\n",(0,o.jsx)(t.p,{children:"\u5bf9\u4e8e\u6e90\u4ee3\u7801\u7684\u5f62\u5f0f\uff0cmake_ir\u7684\u8fc7\u7a0b\u662fmake_ir->ast_to_ttir->ast.parse->generator.visit\uff0c\u5728\u8fd9\u4e00\u8fc7\u7a0b\u4e2dast.parse\u4f1a\u628a\u6e90\u4ee3\u7801\u89e3\u6790\u6210\u62bd\u8c61\u8bed\u6cd5\u6811(AST)\uff0c\u5176\u4e2d\u6811\u7684\u8282\u70b9\u5305\u542b\u4e86\u6e90\u4ee3\u7801\u7684\u5404\u79cd\u62bd\u8c61\u6a21\u5757\uff0c\u6bd4\u5982module\u3001function\u3001args\u3001attr\u3001name\u3001op\u7b49\u7b49\u3002\u800c\u5728generator.visit\u9636\u6bb5\uff0c\u4f1a\u5bf9\u8fd9\u4e9b\u8282\u70b9\u8fdb\u884c\u904d\u5386lower\u3002"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:"def visit_BinOp(self, node):\n    lhs = self.visit(node.left)\n    rhs = self.visit(node.right)\n    method_name = self._method_name_for_bin_op.get(type(node.op))\n    if method_name is None:\n        raise self._unsupported(node,\n                                \"AST binary operator '{}' is not (currently) implemented.\".format(node.op.__name__))\n    return self._apply_binary_method(method_name, lhs, rhs)\n"})}),"\n",(0,o.jsx)(t.p,{children:'\u6211\u4eec\u7f16\u8bd1\u7684\u540e\u7aefnvptx\u53c8\u5c06\u8fd9\u4e00\u6b65\u5206\u4e3a\u4e86\u4e94\u4e2a\u5c0f\u9636\u6bb5\uff1amake_ttir\u3001make_ttgir\u3001make_llir\u3001make_ptx\u548cmake_cubin\uff0c\u5176\u4e2d\u540e\u4e24\u4e2a\u9636\u6bb5\u501f\u52a9llvm\u548cnv\u7684ptxas\u5b8c\u6210\uff0c\u56e0\u6b64\u6211\u4eec\u4e3b\u8981\u5173\u6ce8\u524d\u9762\u4e09\u4e2a\u9636\u6bb5\u3002\u8fd9\u4e9b\u9636\u6bb5\u7531\u591a\u4e2apass\u7ec4\u5408\u800c\u6210\uff08\u8fd9\u91cc\u9ed8\u8ba4\u8bfb\u8005\u77e5\u6653\u4e86\u7f16\u8bd1\u5668\u4e2d"pass"\u7684\u542b\u4e49\u548c\u4f5c\u7528\uff09\uff0c \u6839\u636epass\u7684\u6765\u6e90\uff0c\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u5b83\u4eec\u5927\u81f4\u53ef\u4ee5\u5206\u4e3a\u4e0b\u9762\u51e0\u79cd'}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"common\uff0c\u5b9a\u4e49\u5728mlir/include/mlir/Transforms/Passes.td"}),"\n",(0,o.jsx)(t.li,{children:"ttir\uff0c\u5b9a\u4e49\u5728triton/include/triton/Dialect/Triton/Transforms/Passes.td"}),"\n",(0,o.jsx)(t.li,{children:"ttgpuir\uff0c\u5b9a\u4e49\u5728triton/include/triton/Dialect/TritonGPU/Transforms/Passes.td"}),"\n",(0,o.jsx)(t.li,{children:"ttnvgpuir\uff0c\u5b9a\u4e49\u5728triton/include/triton/Dialect/TritonNvidiaGPU/Transforms/Passes.td\n\u5b83\u4eec\u90fd\u901a\u8fc7\u7ee7\u627fmlir/Pass/PassBase.td\u6765\u5b9a\u4e49Pass\u3002\u4ece\u4e0a\u5230\u4e0b\u5b83\u4eec\u7684\u8bed\u4e49\u8d8a\u6765\u8d8a\u5e95\u5c42\uff0c\u548c\u786c\u4ef6\u76f8\u5173\u7684\u4fe1\u606f\u4e5f\u8d8a\u591a\u3002"}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"make_ttir",children:"make_ttir"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:"@staticmethod\ndef make_ttir(mod, metadata, opt):\n    pm = ir.pass_manager(mod.context)\n    pm.enable_debug()             \n    passes.common.add_inliner(pm) # try to inline function call\n    passes.ttir.add_rewrite_tensor_pointer(pm) # Rewrite load/stores with tensor pointers into legacy load/stores\n    passes.ttir.add_combine(pm) # combine ops\n    passes.common.add_canonicalizer(pm) # converts operations into their canonical forms by folding constants, identity transformations etc.\n    passes.ttir.add_reorder_broadcast(pm) # Moves broadcast and splat after elementwise operations\n    passes.common.add_cse(pm) # Eliminate common sub-expressions\n    passes.common.add_licm(pm) # Hoist loop invariant instructions outside of the loop\n    passes.common.add_symbol_dce(pm) # Eliminate dead symbols\n    pm.run(mod)\n    return mod\n"})}),"\n",(0,o.jsx)(t.p,{children:"\u8fd9\u91cc\u57fa\u672c\u90fd\u662f\u4e00\u4e9b\u4f18\u5316pass\uff0c\u4f8b\u5982add_inliner\uff0c\u5728\u8c03\u7528add_inliner pass\u65f6\u4f1a\u901a\u8fc7pass.cc\u8df3\u8f6c\u5230Passes.td\u91cc\u5b9a\u4e49\u7684constructor \uff0c\u4ece\u800c\u8fdb\u5165pass\u7684\u5177\u4f53\u5b9e\u73b0\u6587\u4ef6\u3002"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'def Inliner : Pass<"inline"> {\n  let summary = "Inline function calls";\n  let constructor = "mlir::createInlinerPass()";\n  let options = [\n   ......\n  ];\n}\n'})}),"\n",(0,o.jsx)(t.h2,{id:"make_ttgir",children:"make_ttgir"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'@staticmethod\ndef make_ttgir(mod, metadata, opt, capability):\n    cluster_info = nvidia.ClusterInfo()\n    if opt.cluster_dims is not None:\n        cluster_info.clusterDimX = opt.cluster_dims[0]\n        cluster_info.clusterDimY = opt.cluster_dims[1]\n        cluster_info.clusterDimZ = opt.cluster_dims[2]\n    # TTIR -> TTGIR\n    pm = ir.pass_manager(mod.context)\n    pm.enable_debug()\n    passes.ttir.add_convert_to_ttgpuir(pm, opt.num_warps, 32, opt.num_ctas, capability) \n    # optimize TTGIR\n    passes.ttgpuir.add_coalesce(pm) # coalesced some mem op for cache optimization\n    nvidia.passes.ttnvgpuir.add_plan_cta(pm, cluster_info) # \u51b3\u5b9aCTA(threadblock)\u5206\u5757\n    passes.ttgpuir.add_remove_layout_conversions(pm) # remove superfluous layout conversions\n    passes.ttgpuir.add_optimize_thread_locality(pm) # Reduce the cost of synchronization between threads in an SM\n    passes.ttgpuir.add_accelerate_matmul(pm, capability) # Optimize the input/output layout of `dot` instruction to make them compatible hardware accelerators(e.g., Nvidia tensor cores)\n    passes.ttgpuir.add_remove_layout_conversions(pm) # remove superfluous layout conversions\n    passes.ttgpuir.add_optimize_dot_operands(pm) # Re-arranged layouts of tensors used as matrix multiplication operands\n    passes.common.add_cse(pm)  # Eliminate common sub-expressions\n    if capability // 10 >= 8:\n        passes.ttgpuir.add_pipeline(pm, opt.num_stages, opt.num_warps, opt.num_ctas, capability) # Applies software pipelining to loops in the module based on number of stages\n    if capability // 10 <= 8:\n        passes.ttgpuir.add_prefetch(pm) # Decompose `DotOp` instructions in loops into several finer-grained `DotOp`\n    passes.ttgpuir.add_optimize_dot_operands(pm)  # Re-arranged layouts of tensors used as matrix multiplication operands\n    passes.ttgpuir.add_remove_layout_conversions(pm) # remove superfluous layout conversions\n    passes.ttgpuir.add_reduce_data_duplication(pm) # Reduce data duplication in register by decomposing\n    passes.ttgpuir.add_reorder_instructions(pm) # Reorder instructions\n    passes.common.add_cse(pm) # Eliminate common sub-expressions\n    passes.common.add_symbol_dce(pm) # Eliminate dead symbols\n    if capability // 10 >= 9:\n        nvidia.passes.ttnvgpuir.add_fence_insertion(pm) # Insert fences across generic and async proxy\n    passes.common.add_canonicalizer(pm) # converts operations into their canonical forms by folding constants, identity transformations etc.\n    pm.run(mod)\n    metadata["cluster_dims"] = (cluster_info.clusterDimX, cluster_info.clusterDimY, cluster_info.clusterDimZ)\n    return mod\n'})}),"\n",(0,o.jsx)(t.p,{children:"\u8fd9\u4e2a\u8f6c\u6362\u8fc7\u7a0b\u4e5f\u53ef\u4ee5\u53eb\u505arewrite\u8fc7\u7a0b\uff0c\u8fd9\u91cc\u6211\u4eec\u5148\u6ce8\u610f\u4e00\u4e0b\u7528\u5230\u7684type converter\u548ctarget\u548c\u5206\u522b\u662fTritonGPUTypeConverter\u548cTritonGPUConversionTarget\u3002\u8fd9\u4e24\u4e2a\u5bf9\u8c61\u975e\u5e38\u91cd\u8981\uff0ctype converter\u4f1a\u6307\u5b9a\u67d0\u4e9b\u6570\u636e\u7c7b\u578b\u7684\u8f6c\u6362\u65b9\u5f0f\uff0ctarget\u4f1a\u6307\u5b9a\u54ea\u4e9bop\u662f\u5408\u6cd5\u7684\uff08\u7c7b\u4f3cLLVM\u4e2d\u7684\u7c7b\u578b\u5408\u6cd5\u5316\u548cop\u5408\u6cd5\u5316\uff09\uff0c\u540e\u9762\u4e5f\u4f1a\u4ecb\u7ecd\u5b83\u4eec\u7684\u4ee3\u7801\u3002"}),"\n",(0,o.jsx)(t.h2,{id:"make_llir",children:"make_llir"}),"\n",(0,o.jsx)(t.p,{children:"\u6839\u636e\u6ce8\u91ca\uff0c\u8fd9\u4e00\u6b65\u5176\u5b9e\u53c8\u53ef\u4ee5\u5206\u4e3a\u4e24\u5c0f\u6b65\uff0cTritonGPU -> LLVM-IR (MLIR) \u548c LLVM-IR (MLIR) -> LLVM-IR (LLVM)\u3002\u8fd9\u4e24\u6b65\u7684\u533a\u522b\u5728\u4e8e\uff0c\u7b2c\u4e00\u6b65\u662f\u8fd8\u662fMLIR\u7ea7\u522b\u7684\uff0c\u4e5f\u5c31\u662f\u5728dialect\u7a7a\u95f4\u7684\u8f6c\u6362\uff0c\u8f6c\u6362\u7684\u7ed3\u679c\u5c31\u662fLLVMDialect\uff0c\u800c\u7b2c\u4e8c\u6b65\u662f\u5c06LLVMDialect\u8f6c\u6362\u4e3a\u771f\u6b63\u7684LLVM IR\u3002"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'   @staticmethod\n    def make_llir(src, metadata, options, capability):\n        # warp-specialization mutates num_warps\n        num_warp_groups = src.get_int_attr("triton_gpu.num-warp-groups-per-cta")\n        if num_warp_groups is not None:\n            metadata["num_warps"] *= num_warp_groups\n        mod = src\n        # TritonGPU -> LLVM-IR (MLIR)\n        pm = ir.pass_manager(mod.context)\n        pm.enable_debug()\n        nvidia.passes.ttgpuir.add_decompose_unsupported_conversions(pm) # Decompose conversions that are not supported by TritonGPU -> LLVM\n        passes.convert.add_scf_to_cf(pm) # Convert SCF dialect to ControlFlow dialect\n        passes.convert.add_index_to_llvmir(pm) # Lower the `index` dialect to the `llvm` dialect\n        passes.ttgpuir.add_allocate_shared_memory(pm) # Add metadata for shared memory allocation\n        nvidia.passes.ttgpuir.add_to_llvmir(pm, capability)\n        nvidia.passes.ttnvgpuir.add_nvgpu_to_llvm(pm) # \u7528\u6765\u5904\u7406NVGPUDialect\u7684\u8282\u70b9\uff0c\u5927\u90e8\u5206\u66ff\u6362\u4e3a\u5185\u5d4c\u6c47\u7f16\n        passes.convert.add_arith_to_llvmir(pm) # Convert Arith dialect to LLVM dialect\n        passes.common.add_canonicalizer(pm) # converts operations into their canonical forms by folding constants, identity transformations etc.\n        passes.common.add_cse(pm) # Eliminate common sub-expressions\n        passes.common.add_symbol_dce(pm) # Eliminate dead symbols\n        if os.environ.get("TRITON_DISABLE_LINE_INFO", "0") == "0":\n            passes.llvmir.add_di_scope(pm) # Materialize LLVM line info\n        pm.run(mod)\n        # LLVM-IR (MLIR) -> LLVM-IR (LLVM) \n        llvm.init_targets()\n        context = llvm.context()\n        llvm_mod = llvm.to_module(mod, context) # \u5c06LLVM dialect\u8f6c\u6362\u4e3aLLVMIR\n        nvidia.set_nvvm_reflect_ftz(llvm_mod) # enable fast math path in libdevice\n        if options.extern_libs:\n            for name, path in options.extern_libs:\n                llvm.link_extern_lib(llvm_mod, path) # link libdevice\uff0c\u4e00\u4e9b\u51fd\u6570\u5e93\u4f1a\u7528\u5230\n        llvm.optimize_module(llvm_mod, llvm.OPTIMIZE_O3) # O3\u4f18\u5316\n        metadata["shared"] = src.get_int_attr("triton_gpu.shared")\n        ret = str(llvm_mod)\n        del llvm_mod\n        del context\n        return ret\n'})}),"\n",(0,o.jsx)(t.p,{children:"\u7531\u4e8e\u7b2c\u4e8c\u6b65\u7684\u8f6c\u6362\u6bd4\u8f83\u56fa\u5b9a\uff0c\u6211\u4eec\u91cd\u70b9\u5173\u6ce8\u7b2c\u4e00\u6b65\uff0c\u5c06\u5404\u79cddialect\u90fd\u8f6c\u6210LLVMDialect\u3002\u5176\u4e2d\u4e3b\u8981\u5173\u6ce8add_to_llvmir\u8fd9\u4e2apass\uff0c\u56e0\u4e3a\u6211\u4eeccase\u4e2d\u7684arith.addi\u3001tt.load\u548ctt.store\u90fd\u4f1a\u5728\u8fd9\u4e2apass\u4e2d\u88ab\u91cd\u5199"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:"void runOnOperation() override {\n    MLIRContext *context = &getContext();\n    ModuleOp mod = getOperation();\n    mlir::LowerToLLVMOptions option(context);\n    option.overrideIndexBitwidth(32);\n    TritonGPUToLLVMTypeConverter typeConverter(context, option);\n    TritonLLVMConversionTarget convTarget(*context);\n    int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n    int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(mod);\n    int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n\n    // Allocate shared memory and set barrier\n    ModuleAllocation allocation(mod);\n    ModuleMembarAnalysis membarPass(&allocation);\n    membarPass.run();\n\n    // Lower functions\n    {\n      mlir::LowerToLLVMOptions option(context);\n      TritonGPUToLLVMTypeConverter typeConverter(context, option);\n      TritonLLVMFunctionConversionTarget funcTarget(*context);\n      RewritePatternSet funcPatterns(context);\n      funcPatterns.add<FuncOpConversion>(typeConverter, numWarps,\n                                         patternBenefitDefault);\n      mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n                                                            funcPatterns);\n      if (failed(\n              applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n        return signalPassFailure();\n    }\n\n    // initSharedMemory is run before the conversion of call and ret ops,\n    // because the call op has to know the shared memory base address of each\n    // function\n    initSharedMemory(typeConverter);\n    ModuleAxisInfoAnalysis axisInfoAnalysis(mod);\n    OpBuilder::InsertPoint indexInsertPoint;\n\n    RewritePatternSet patterns(context);\n    TargetInfo targetInfo(computeCapability);\n    int benefit = patternBenefitPrioritizeOverLLVMConversions;\n    ......\n    populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, axisInfoAnalysis,\n                                      benefit);\n    // \u4f1a\u8c03\u7528\u4e0b\u9762\u4e24\u6761\n    // patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n    // patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n    mlir::arith::populateArithToLLVMConversionPatterns(typeConverter, patterns);\n    // \u4f1a\u8c03\u7528patterns.add<AddIOpLowering...>\n    ......\n    if (failed(applyPartialConversion(mod, convTarget, std::move(patterns))))\n      return signalPassFailure();\n    ......\n  }\n"})}),"\n",(0,o.jsx)(t.p,{children:"\u6211\u4eec\u53d1\u73b0\uff0c\u8fd9\u91cc\u7684\u4ee3\u7801\u7ed3\u6784\u4e5f\u662fpopulate#Opname#Pattern\uff0c\u7136\u540e\u518d\u6267\u884capplyPartialConversion\uff0c\u597d\u50cf\u548c\u4e0a\u4e00\u7ae0\u4e2dadd_convert_to_ttgpuir\u7684\u8f6c\u6362\u8fc7\u7a0b\u5dee\u4e0d\u591a\uff0c\u7ec6\u5fc3\u7684\u5c0f\u4f19\u4f34\u53ef\u4ee5\u89c2\u5bdf\u5230\u533a\u522b\u5728\u4e8e\u8fd9\u91cc\u6211\u4eec\u7684target\u662fTritonLLVMFunctionConversionTarget\uff0c\u800c\u524d\u9762\u662fTritonGPUConversionTarget\uff0c\u524d\u8005\u53c8\u589e\u52a0\u4e86IndexDialect\u3001LLVMDialect\u3001NVMDialect\u7b49\u4e3a\u5408\u6cd5\uff0c\u5176\u4ed6\u975e\u6cd5dialect\u5728\u8fd9\u4e00\u9636\u6bb5\u4f1a\u88ablower\u3002"})]})}function p(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}}}]);