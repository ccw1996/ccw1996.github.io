"use strict";(self.webpackChunkblog_sample=self.webpackChunkblog_sample||[]).push([[550],{5223:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>p});const o=JSON.parse('{"id":"LLM/export_clip_onnx","title":"clip\u5bfc\u51fa\u4e3aonnx","description":"clip\u5bfc\u51fa\u4e3aonnx","source":"@site/docs/LLM/export_clip_onnx.md","sourceDirName":"LLM","slug":"/LLM/export_clip_onnx","permalink":"/docs/LLM/export_clip_onnx","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/edit/main/website/docs/LLM/export_clip_onnx.md","tags":[{"inline":true,"label":"work","permalink":"/docs/tags/work"}],"version":"current","lastUpdatedAt":1740785860000,"frontMatter":{"title":"clip\u5bfc\u51fa\u4e3aonnx","description":"clip\u5bfc\u51fa\u4e3aonnx","tags":["work"],"editor":"caroot"},"sidebar":"tutorialSidebar","previous":{"title":"DINOv2","permalink":"/docs/LLM/Dinov2"},"next":{"title":"\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u65f6\u95f4\uff1a\u4ece\u4f30\u7b97\u5230 FLOPs \u63a8\u5bfc","permalink":"/docs/LLM/flop_calc"}}');var i=t(4848),r=t(8453);const a={title:"clip\u5bfc\u51fa\u4e3aonnx",description:"clip\u5bfc\u51fa\u4e3aonnx",tags:["work"],editor:"caroot"},l=void 0,s={},p=[{value:"\u9488\u5bf9image\u548ctext\u5355\u72ec\u63d0\u53d6onnx",id:"\u9488\u5bf9image\u548ctext\u5355\u72ec\u63d0\u53d6onnx",level:2},{value:"\u5408\u5e76\u6210\u4e00\u4e2a\u5b8c\u6574\u7684onnx",id:"\u5408\u5e76\u6210\u4e00\u4e2a\u5b8c\u6574\u7684onnx",level:2}];function c(e){const n={code:"code",h2:"h2",pre:"pre",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"\u9488\u5bf9image\u548ctext\u5355\u72ec\u63d0\u53d6onnx",children:"\u9488\u5bf9image\u548ctext\u5355\u72ec\u63d0\u53d6onnx"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport clip\nfrom PIL import Image\n \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n \nmodel.float()\nmodel.eval()\n \nimage = preprocess(Image.open(\"clip_dog.png\")).unsqueeze(0).to(device)\ntext = clip.tokenize([\"a dog\", \"a cat\"]).to(device)\n \nprint(\"text:\", text)\n \nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n \n    logits_per_image, logits_per_text = model(image, text)\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n \nprint(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]\n \n# export to ONNX\n \n \nclass ImgModelWrapper(nn.Module):\n    def __init__(self):\n        super(ImgModelWrapper, self).__init__()\n        self.model = model\n \n    def forward(self, image):\n        image_features = model.encode_image(image)\n        return image_features\n \n \nclass TxtModelWrapper(nn.Module):\n    def __init__(self):\n        super(TxtModelWrapper, self).__init__()\n        self.model = model\n \n    def forward(self, image):\n        text_features = model.encode_text(text)\n        return text_features\n \n \nimg_model = ImgModelWrapper()\ntxt_model = TxtModelWrapper()\n \ntorch.onnx.export(img_model,               # model being run\n                  image,                         # model input (or a tuple for multiple inputs)\n                  \"openai_vit_img.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=12,          # the ONNX version to export the model to\n                  do_constant_folding=False,  # whether to execute constant folding for optimization\n                  input_names=['input'],   # the model's input names\n                  output_names=['output'],  # the model's output names\n                  dynamic_axes={'input': {0: 'batch'}})\ntorch.onnx.export(txt_model,               # model being run\n                  text,                         # model input (or a tuple for multiple inputs)\n                  \"openai_vit_txt.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=12,          # the ONNX version to export the model to\n                  do_constant_folding=False,  # whether to execute constant folding for optimization\n                  input_names=['input'],   # the model's input names\n                  output_names=['output'],  # the model's output names\n                  dynamic_axes={'input': {0: 'batch'}})\n"})}),"\n",(0,i.jsx)(n.h2,{id:"\u5408\u5e76\u6210\u4e00\u4e2a\u5b8c\u6574\u7684onnx",children:"\u5408\u5e76\u6210\u4e00\u4e2a\u5b8c\u6574\u7684onnx"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import clip\nimport torch\nfrom PIL import Image\n\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nmodel, preprocess = clip.load("ViT-B/32", device=device)\n \nmodel.float()\nmodel.eval()\nnpx=model.visual.input_resolution\nimage = preprocess(Image.open("clip_dog.png")).unsqueeze(0).to(device)\ntext = clip.tokenize(["a dog", "a cat"]).to(device)\n\nmodel.forward(image,text)\ntorch.onnx.export(model,(image,text),"clip.onnx",export_params=True,input_names=["IMAGE", "TEXT"],\n  output_names=["LOGITS_PER_IMAGE", "LOGITS_PER_TEXT"],\n  opset_version=14,\n  dynamic_axes={\n      "IMAGE": {\n          0: "image_batch_size",\n      },\n      "TEXT": {\n          0: "text_batch_size",\n      },\n      "LOGITS_PER_IMAGE": {\n          0: "image_batch_size",\n          1: "text_batch_size",\n      },\n      "LOGITS_PER_TEXT": {\n          0: "text_batch_size",\n          1: "image_batch_size",\n      },\n  })\n'})})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>l});var o=t(6540);const i={},r=o.createContext(i);function a(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);